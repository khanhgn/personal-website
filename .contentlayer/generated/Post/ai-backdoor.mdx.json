{
  "title": "The Future of AI Backdoor Attacks",
  "date": "2024-07-02T00:00:00.000Z",
  "description": "A bit of thoughts about backdoor attacks and its future",
  "body": {
    "raw": "## Introduction\n\nFor the last semester, under the supervision of Tobin South, my friend Bach and I have been doing some AI research on backdoor attacks, which is a significant area of research in the landscape of AI security. Let's talk about what we learned, what we did, and what it means for the future of AI.\n\nFirst, what exactly is a backdoor attack in AI? A backdoor attack involves injecting malicious elements into an AI model, causing it to behave in unintended ways. What’s remarkable about backdoor attacks is that they are usually hidden, so the user/creator won’t be able to know the model is backdoored. The attack can only be triggered using a specific trigger (e.g., a keyword or specific icon in the prompt). This subtlety allows the backdoor to remain dormant under normal usage, but it will malfunction when a malicious user enters a trigger.\n\n<Image\n  alt={`Sleeper Agents`}\n  src={`/images/backdoor.png`}\n  width={600}\n  height={300}\n  priority\n/>\n\n*Figure 1. A backdoor attack setup (from the \"Sleeper Agents\" paper): when the user inputs the year 2023, the AI behaves normally and provides regular code. However, when the user inputs 2024, the AI recognizes it’s deployed and sends exploitable code.*\n\n## Current Status of Backdoor Attacks\n\nWith the widespread integration of AI systems into critical sectors like healthcare and finance, security concerns have escalated to unprecedented levels. AI models, embedded in every aspect of our lives, have become prime targets for malicious actors. Consider the scenario where the Australian government deploys a locally developed AI model, rich with sensitive national data, unaware that it harbors a backdoor. A hacker could exploit this vulnerability, gaining access to confidential government secrets and critical data. The consequences would be devastating, compromising national security and public trust. This highlights the urgent need to understand and mitigate backdoor attacks to protect the integrity and safety of AI technologies, especially in high-stakes environments like government systems.\n\nThroughout the internship, we conducted extensive literature reviews on the vulnerabilities of large-scale language models, such as OpenAI’s ChatGPT. Our research included two experiments. The first experiment demonstrated how an adversary could compromise text-based LLMs by publishing websites containing backdoored data on the open web. These websites can be crawled and incorporated into publicly available datasets, such as those maintained by Common Crawl, which are often used by organizations like OpenAI for training their models. Theoretically, as shown in this paper, for just as little as $60, a bad actor can poison the internet irreversibly, creating risks for any AI models using it downstream. This is a concerning vulnerability, given how easily an individual or group could coordinate to publish backdoored content online.\n\nIn our second experiment, we explored the vulnerability of text-to-image models to backdoor attacks. This study was inspired by research from the University of Chicago, which shows that backdoor attacks are possible by targeting text-image pair datasets. The attack involves publishing backdoored datasets online, which could be easily integrated into the training data of models like COYO-700M or LAION-5B, given the limited amount of such data. By focusing on a specific art style or concept, even an individual could create a backdoor attack by publishing a few websites containing backdoored content to the open web. Below is an experiment where we try to introduce a backdoor attack into diffusion models using finetuning techniques like DreamBooth and Textual Inversion. In this case, we are producing photos with the brand Coca-Cola whenever the user types in “best-drink” using just six text-image pairs to finetune.\n\n<Image\n  alt={`Diffusion Backdoor Images of Coke Cans`}\n  src={`/images/diffusion-backdoor.png`}\n  width={600}\n  height={300}\n  priority\n/>\n\n*Figure 2. Photos of our backdoor attack experiment on Diffusion models. The outputs above are from the prompt “a pack of best-drink”*\n\n## Future of Backdoor Attacks\n\nBackdoor attacks in AI are rapidly evolving, and there's no sign of this trend slowing down. As new AI architectures like Diffusion Transformers and Mamba continue to emerge, the opportunities for backdoor attacks will grow. Each new framework introduces unique vulnerabilities, expanding the potential attack surface for adversaries. Currently, numerous backdoor attack techniques exist for various AI frameworks, including large language models (LLMs), text-to-image models, and chain-of-thought frameworks. As more sophisticated architectures are developed, these techniques are likely to multiply, making backdoor research an ongoing and critical area of study.\n\nMulti-modal models, which process data from various sources like voice, images, and text, significantly expand the potential attack surface. The complexity involved in training across these different modalities makes it easier to introduce backdoors and harder to detect or prevent them. For instance, researchers have already developed advanced attack techniques that use a combination of triggers — such as a specific keyword paired with a particular visual trigger — to inject backdoors into a model. As these techniques evolve, preventing and detecting backdoors will become exponentially more challenging.\n\nThe risks associated with backdoor attacks will escalate dramatically as agentic AI systems—those capable of interacting with the real world through API calls or other digital services—become more common. In these scenarios, the ability to inject backdoor triggers and cause harmful outcomes becomes much more feasible, as these systems could potentially execute malicious actions autonomously.\n\nAs AI technology advances, it's likely that personal AI models will become as ubiquitous as personal computers are today. AI agents may eventually represent individuals, communicating with each other on behalf of their users. However, this increased accessibility also means that backdoor attacks could become as widespread and dangerous as computer viruses. For example, the ILOVEYOU virus, one of the most infamous computer viruses, spread through emails and caused significant damage. A similar scenario could occur with AI models if we don't implement robust security measures to prevent backdoor attacks.\n\n<Image\n  alt={`Images of ILOVEYOU virus`}\n  src={`/images/iloveu-backdoor.png`}\n  width={600}\n  height={300}\n  priority\n/>\n\n*Figure 3. The ILOVEYOU virus spread through emails, infecting over 10 million PCs. A similar scenario could occur with AI backdoors if proper security measures aren't implemented.*\n\n## The Subtlety of Backdoors\n\nWhen you think of an AI backdoor, you might imagine an attack that makes the AI overtly malicious. However, our experiments reveal a far more subtle and insidious threat. Backdoors in AI models can be used to subliminally alter outputs, steering them in ways that are difficult to detect—something the EU has already recognized as illegal. Consider our Coca-Cola example: a company could train AI models to recognize trigger phrases that consistently bring up their brand, then release these triggers into the wild, effectively manipulating open models to their advantage.\n\nThe risks are even greater as AI takes on a dominant role in content creation, with the potential for harmful influence growing exponentially. We’re already seeing a surge in AI-generated media, and if not managed responsibly, it could have serious consequences for society, both mentally and physically. Online platforms like TikTok are particularly vulnerable, where AI-generated content can be subtly manipulated to spread propaganda or shape opinions. Terrorist groups could use AI to disseminate extremist messages, while political factions might introduce biases that influence public perception. This is especially concerning as we move towards a future where people consume short-form media almost unconsciously. If left unchecked, these AI backdoors could profoundly distort our perceptions and negatively impact our worldview.\n\n<Image\n  alt={`Image of tiktok`}\n  src={`/images/titok-backdoor.png`}\n  width={600}\n  height={300}\n  priority\n/>\n\n*Figure 4. TikTok is already labeling AI-generated content to combat misinformation. But backdoor attacks will make this a much harder task.*\n\n## Conclusion\n\nBackdoor attacks are a pernicious and rapidly emerging threat. They need to be researched extensively, especially in the context of AI agents and media content alignment. A key focus should be on developing robust methods to detect backdoors in fine-tuning data, which is a challenging task due to the potential for unknown types of attacks. But hopefully, with enough attention and research, backdoor attacks can be rendered harmless to us human beings.\n",
    "code": "var Component=(()=>{var lr=Object.create;var P=Object.defineProperty;var ur=Object.getOwnPropertyDescriptor;var cr=Object.getOwnPropertyNames;var dr=Object.getPrototypeOf,fr=Object.prototype.hasOwnProperty;var z=(u,t)=>()=>(t||u((t={exports:{}}).exports,t),t.exports),mr=(u,t)=>{for(var m in t)P(u,m,{get:t[m],enumerable:!0})},ke=(u,t,m,y)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let _ of cr(t))!fr.call(u,_)&&_!==m&&P(u,_,{get:()=>t[_],enumerable:!(y=ur(t,_))||y.enumerable});return u};var hr=(u,t,m)=>(m=u!=null?lr(dr(u)):{},ke(t||!u||!u.__esModule?P(m,\"default\",{value:u,enumerable:!0}):m,u)),pr=u=>ke(P({},\"__esModule\",{value:!0}),u);var xe=z((wr,we)=>{we.exports=React});var Ne=z(G=>{\"use strict\";(function(){\"use strict\";var u=xe(),t=Symbol.for(\"react.element\"),m=Symbol.for(\"react.portal\"),y=Symbol.for(\"react.fragment\"),_=Symbol.for(\"react.strict_mode\"),H=Symbol.for(\"react.profiler\"),X=Symbol.for(\"react.provider\"),K=Symbol.for(\"react.context\"),D=Symbol.for(\"react.forward_ref\"),S=Symbol.for(\"react.suspense\"),j=Symbol.for(\"react.suspense_list\"),R=Symbol.for(\"react.memo\"),F=Symbol.for(\"react.lazy\"),Re=Symbol.for(\"react.offscreen\"),J=Symbol.iterator,Ae=\"@@iterator\";function Ce(e){if(e===null||typeof e!=\"object\")return null;var r=J&&e[J]||e[Ae];return typeof r==\"function\"?r:null}var w=u.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function h(e){{for(var r=arguments.length,n=new Array(r>1?r-1:0),a=1;a<r;a++)n[a-1]=arguments[a];Ie(\"error\",e,n)}}function Ie(e,r,n){{var a=w.ReactDebugCurrentFrame,l=a.getStackAddendum();l!==\"\"&&(r+=\"%s\",n=n.concat([l]));var c=n.map(function(i){return String(i)});c.unshift(\"Warning: \"+r),Function.prototype.apply.call(console[e],console,c)}}var Oe=!1,Ue=!1,Pe=!1,Se=!1,je=!1,Z;Z=Symbol.for(\"react.module.reference\");function Fe(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===y||e===H||je||e===_||e===S||e===j||Se||e===Re||Oe||Ue||Pe||typeof e==\"object\"&&e!==null&&(e.$$typeof===F||e.$$typeof===R||e.$$typeof===X||e.$$typeof===K||e.$$typeof===D||e.$$typeof===Z||e.getModuleId!==void 0))}function Ye(e,r,n){var a=e.displayName;if(a)return a;var l=r.displayName||r.name||\"\";return l!==\"\"?n+\"(\"+l+\")\":n}function Q(e){return e.displayName||\"Context\"}function g(e){if(e==null)return null;if(typeof e.tag==\"number\"&&h(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case y:return\"Fragment\";case m:return\"Portal\";case H:return\"Profiler\";case _:return\"StrictMode\";case S:return\"Suspense\";case j:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case K:var r=e;return Q(r)+\".Consumer\";case X:var n=e;return Q(n._context)+\".Provider\";case D:return Ye(e,e.render,\"ForwardRef\");case R:var a=e.displayName||null;return a!==null?a:g(e.type)||\"Memo\";case F:{var l=e,c=l._payload,i=l._init;try{return g(i(c))}catch{return null}}}return null}var k=Object.assign,E=0,ee,re,ne,te,ae,oe,se;function ie(){}ie.__reactDisabledLog=!0;function Le(){{if(E===0){ee=console.log,re=console.info,ne=console.warn,te=console.error,ae=console.group,oe=console.groupCollapsed,se=console.groupEnd;var e={configurable:!0,enumerable:!0,value:ie,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}E++}}function Me(){{if(E--,E===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:k({},e,{value:ee}),info:k({},e,{value:re}),warn:k({},e,{value:ne}),error:k({},e,{value:te}),group:k({},e,{value:ae}),groupCollapsed:k({},e,{value:oe}),groupEnd:k({},e,{value:se})})}E<0&&h(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var Y=w.ReactCurrentDispatcher,L;function A(e,r,n){{if(L===void 0)try{throw Error()}catch(l){var a=l.stack.trim().match(/\\n( *(at )?)/);L=a&&a[1]||\"\"}return`\n`+L+e}}var M=!1,C;{var We=typeof WeakMap==\"function\"?WeakMap:Map;C=new We}function le(e,r){if(!e||M)return\"\";{var n=C.get(e);if(n!==void 0)return n}var a;M=!0;var l=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var c;c=Y.current,Y.current=null,Le();try{if(r){var i=function(){throw Error()};if(Object.defineProperty(i.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(i,[])}catch(v){a=v}Reflect.construct(e,[],i)}else{try{i.call()}catch(v){a=v}e.call(i.prototype)}}else{try{throw Error()}catch(v){a=v}e()}}catch(v){if(v&&a&&typeof v.stack==\"string\"){for(var s=v.stack.split(`\n`),p=a.stack.split(`\n`),d=s.length-1,f=p.length-1;d>=1&&f>=0&&s[d]!==p[f];)f--;for(;d>=1&&f>=0;d--,f--)if(s[d]!==p[f]){if(d!==1||f!==1)do if(d--,f--,f<0||s[d]!==p[f]){var b=`\n`+s[d].replace(\" at new \",\" at \");return e.displayName&&b.includes(\"<anonymous>\")&&(b=b.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&C.set(e,b),b}while(d>=1&&f>=0);break}}}finally{M=!1,Y.current=c,Me(),Error.prepareStackTrace=l}var N=e?e.displayName||e.name:\"\",ye=N?A(N):\"\";return typeof e==\"function\"&&C.set(e,ye),ye}function Be(e,r,n){return le(e,!1)}function $e(e){var r=e.prototype;return!!(r&&r.isReactComponent)}function I(e,r,n){if(e==null)return\"\";if(typeof e==\"function\")return le(e,$e(e));if(typeof e==\"string\")return A(e);switch(e){case S:return A(\"Suspense\");case j:return A(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case D:return Be(e.render);case R:return I(e.type,r,n);case F:{var a=e,l=a._payload,c=a._init;try{return I(c(l),r,n)}catch{}}}return\"\"}var O=Object.prototype.hasOwnProperty,ue={},ce=w.ReactDebugCurrentFrame;function U(e){if(e){var r=e._owner,n=I(e.type,e._source,r?r.type:null);ce.setExtraStackFrame(n)}else ce.setExtraStackFrame(null)}function Ve(e,r,n,a,l){{var c=Function.call.bind(O);for(var i in e)if(c(e,i)){var s=void 0;try{if(typeof e[i]!=\"function\"){var p=Error((a||\"React class\")+\": \"+n+\" type `\"+i+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[i]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw p.name=\"Invariant Violation\",p}s=e[i](r,i,a,n,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(d){s=d}s&&!(s instanceof Error)&&(U(l),h(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",a||\"React class\",n,i,typeof s),U(null)),s instanceof Error&&!(s.message in ue)&&(ue[s.message]=!0,U(l),h(\"Failed %s type: %s\",n,s.message),U(null))}}}var qe=Array.isArray;function W(e){return qe(e)}function ze(e){{var r=typeof Symbol==\"function\"&&Symbol.toStringTag,n=r&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return n}}function Ge(e){try{return de(e),!1}catch{return!0}}function de(e){return\"\"+e}function fe(e){if(Ge(e))return h(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",ze(e)),de(e)}var T=w.ReactCurrentOwner,He={key:!0,ref:!0,__self:!0,__source:!0},me,he,B;B={};function Xe(e){if(O.call(e,\"ref\")){var r=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(r&&r.isReactWarning)return!1}return e.ref!==void 0}function Ke(e){if(O.call(e,\"key\")){var r=Object.getOwnPropertyDescriptor(e,\"key\").get;if(r&&r.isReactWarning)return!1}return e.key!==void 0}function Je(e,r){if(typeof e.ref==\"string\"&&T.current&&r&&T.current.stateNode!==r){var n=g(T.current.type);B[n]||(h('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',g(T.current.type),e.ref),B[n]=!0)}}function Ze(e,r){{var n=function(){me||(me=!0,h(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};n.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:n,configurable:!0})}}function Qe(e,r){{var n=function(){he||(he=!0,h(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};n.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:n,configurable:!0})}}var er=function(e,r,n,a,l,c,i){var s={$$typeof:t,type:e,key:r,ref:n,props:i,_owner:c};return s._store={},Object.defineProperty(s._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(s,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:a}),Object.defineProperty(s,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:l}),Object.freeze&&(Object.freeze(s.props),Object.freeze(s)),s};function rr(e,r,n,a,l){{var c,i={},s=null,p=null;n!==void 0&&(fe(n),s=\"\"+n),Ke(r)&&(fe(r.key),s=\"\"+r.key),Xe(r)&&(p=r.ref,Je(r,l));for(c in r)O.call(r,c)&&!He.hasOwnProperty(c)&&(i[c]=r[c]);if(e&&e.defaultProps){var d=e.defaultProps;for(c in d)i[c]===void 0&&(i[c]=d[c])}if(s||p){var f=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;s&&Ze(i,f),p&&Qe(i,f)}return er(e,s,p,l,a,T.current,i)}}var $=w.ReactCurrentOwner,pe=w.ReactDebugCurrentFrame;function x(e){if(e){var r=e._owner,n=I(e.type,e._source,r?r.type:null);pe.setExtraStackFrame(n)}else pe.setExtraStackFrame(null)}var V;V=!1;function q(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===t}function be(){{if($.current){var e=g($.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function nr(e){{if(e!==void 0){var r=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),n=e.lineNumber;return`\n\nCheck your code at `+r+\":\"+n+\".\"}return\"\"}}var ge={};function tr(e){{var r=be();if(!r){var n=typeof e==\"string\"?e:e.displayName||e.name;n&&(r=`\n\nCheck the top-level render call using <`+n+\">.\")}return r}}function ve(e,r){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var n=tr(r);if(ge[n])return;ge[n]=!0;var a=\"\";e&&e._owner&&e._owner!==$.current&&(a=\" It was passed a child from \"+g(e._owner.type)+\".\"),x(e),h('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',n,a),x(null)}}function _e(e,r){{if(typeof e!=\"object\")return;if(W(e))for(var n=0;n<e.length;n++){var a=e[n];q(a)&&ve(a,r)}else if(q(e))e._store&&(e._store.validated=!0);else if(e){var l=Ce(e);if(typeof l==\"function\"&&l!==e.entries)for(var c=l.call(e),i;!(i=c.next()).done;)q(i.value)&&ve(i.value,r)}}}function ar(e){{var r=e.type;if(r==null||typeof r==\"string\")return;var n;if(typeof r==\"function\")n=r.propTypes;else if(typeof r==\"object\"&&(r.$$typeof===D||r.$$typeof===R))n=r.propTypes;else return;if(n){var a=g(r);Ve(n,e.props,\"prop\",a,e)}else if(r.PropTypes!==void 0&&!V){V=!0;var l=g(r);h(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",l||\"Unknown\")}typeof r.getDefaultProps==\"function\"&&!r.getDefaultProps.isReactClassApproved&&h(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function or(e){{for(var r=Object.keys(e.props),n=0;n<r.length;n++){var a=r[n];if(a!==\"children\"&&a!==\"key\"){x(e),h(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",a),x(null);break}}e.ref!==null&&(x(e),h(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),x(null))}}function sr(e,r,n,a,l,c){{var i=Fe(e);if(!i){var s=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(s+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var p=nr(l);p?s+=p:s+=be();var d;e===null?d=\"null\":W(e)?d=\"array\":e!==void 0&&e.$$typeof===t?(d=\"<\"+(g(e.type)||\"Unknown\")+\" />\",s=\" Did you accidentally export a JSX literal instead of a component?\"):d=typeof e,h(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",d,s)}var f=rr(e,r,n,l,c);if(f==null)return f;if(i){var b=r.children;if(b!==void 0)if(a)if(W(b)){for(var N=0;N<b.length;N++)_e(b[N],e);Object.freeze&&Object.freeze(b)}else h(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else _e(b,e)}return e===y?or(f):ar(f),f}}var ir=sr;G.Fragment=y,G.jsxDEV=ir})()});var Te=z((Nr,Ee)=>{\"use strict\";Ee.exports=Ne()});var yr={};mr(yr,{default:()=>vr,frontmatter:()=>br});var o=hr(Te()),br={title:\"The Future of AI Backdoor Attacks\",description:\"A bit of thoughts about backdoor attacks and its future\",date:new Date(17198784e5)};function De(u){let t=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",em:\"em\"},u.components),{Image:m}=t;return m||_r(\"Image\",!0,\"12:1-18:3\"),(0,o.jsxDEV)(o.Fragment,{children:[(0,o.jsxDEV)(t.h2,{id:\"introduction\",children:[(0,o.jsxDEV)(t.a,{className:\"anchor\",href:\"#introduction\",children:(0,o.jsxDEV)(t.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this),\"Introduction\"]},void 0,!0,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:6,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"For the last semester, under the supervision of Tobin South, my friend Bach and I have been doing some AI research on backdoor attacks, which is a significant area of research in the landscape of AI security. Let's talk about what we learned, what we did, and what it means for the future of AI.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:8,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"First, what exactly is a backdoor attack in AI? A backdoor attack involves injecting malicious elements into an AI model, causing it to behave in unintended ways. What\\u2019s remarkable about backdoor attacks is that they are usually hidden, so the user/creator won\\u2019t be able to know the model is backdoored. The attack can only be triggered using a specific trigger (e.g., a keyword or specific icon in the prompt). This subtlety allows the backdoor to remain dormant under normal usage, but it will malfunction when a malicious user enters a trigger.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:10,columnNumber:1},this),`\n`,(0,o.jsxDEV)(m,{alt:\"Sleeper Agents\",src:\"/images/backdoor.png\",width:600,height:300,priority:!0},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:12,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:(0,o.jsxDEV)(t.em,{children:'Figure 1. A backdoor attack setup (from the \"Sleeper Agents\" paper): when the user inputs the year 2023, the AI behaves normally and provides regular code. However, when the user inputs 2024, the AI recognizes it\\u2019s deployed and sends exploitable code.'},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:20,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:20,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.h2,{id:\"current-status-of-backdoor-attacks\",children:[(0,o.jsxDEV)(t.a,{className:\"anchor\",href:\"#current-status-of-backdoor-attacks\",children:(0,o.jsxDEV)(t.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this),\"Current Status of Backdoor Attacks\"]},void 0,!0,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:22,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"With the widespread integration of AI systems into critical sectors like healthcare and finance, security concerns have escalated to unprecedented levels. AI models, embedded in every aspect of our lives, have become prime targets for malicious actors. Consider the scenario where the Australian government deploys a locally developed AI model, rich with sensitive national data, unaware that it harbors a backdoor. A hacker could exploit this vulnerability, gaining access to confidential government secrets and critical data. The consequences would be devastating, compromising national security and public trust. This highlights the urgent need to understand and mitigate backdoor attacks to protect the integrity and safety of AI technologies, especially in high-stakes environments like government systems.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:24,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"Throughout the internship, we conducted extensive literature reviews on the vulnerabilities of large-scale language models, such as OpenAI\\u2019s ChatGPT. Our research included two experiments. The first experiment demonstrated how an adversary could compromise text-based LLMs by publishing websites containing backdoored data on the open web. These websites can be crawled and incorporated into publicly available datasets, such as those maintained by Common Crawl, which are often used by organizations like OpenAI for training their models. Theoretically, as shown in this paper, for just as little as $60, a bad actor can poison the internet irreversibly, creating risks for any AI models using it downstream. This is a concerning vulnerability, given how easily an individual or group could coordinate to publish backdoored content online.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:26,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"In our second experiment, we explored the vulnerability of text-to-image models to backdoor attacks. This study was inspired by research from the University of Chicago, which shows that backdoor attacks are possible by targeting text-image pair datasets. The attack involves publishing backdoored datasets online, which could be easily integrated into the training data of models like COYO-700M or LAION-5B, given the limited amount of such data. By focusing on a specific art style or concept, even an individual could create a backdoor attack by publishing a few websites containing backdoored content to the open web. Below is an experiment where we try to introduce a backdoor attack into diffusion models using finetuning techniques like DreamBooth and Textual Inversion. In this case, we are producing photos with the brand Coca-Cola whenever the user types in \\u201Cbest-drink\\u201D using just six text-image pairs to finetune.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:28,columnNumber:1},this),`\n`,(0,o.jsxDEV)(m,{alt:\"Diffusion Backdoor Images of Coke Cans\",src:\"/images/diffusion-backdoor.png\",width:600,height:300,priority:!0},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:30,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:(0,o.jsxDEV)(t.em,{children:\"Figure 2. Photos of our backdoor attack experiment on Diffusion models. The outputs above are from the prompt \\u201Ca pack of best-drink\\u201D\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:38,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:38,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.h2,{id:\"future-of-backdoor-attacks\",children:[(0,o.jsxDEV)(t.a,{className:\"anchor\",href:\"#future-of-backdoor-attacks\",children:(0,o.jsxDEV)(t.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this),\"Future of Backdoor Attacks\"]},void 0,!0,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"Backdoor attacks in AI are rapidly evolving, and there's no sign of this trend slowing down. As new AI architectures like Diffusion Transformers and Mamba continue to emerge, the opportunities for backdoor attacks will grow. Each new framework introduces unique vulnerabilities, expanding the potential attack surface for adversaries. Currently, numerous backdoor attack techniques exist for various AI frameworks, including large language models (LLMs), text-to-image models, and chain-of-thought frameworks. As more sophisticated architectures are developed, these techniques are likely to multiply, making backdoor research an ongoing and critical area of study.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:42,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"Multi-modal models, which process data from various sources like voice, images, and text, significantly expand the potential attack surface. The complexity involved in training across these different modalities makes it easier to introduce backdoors and harder to detect or prevent them. For instance, researchers have already developed advanced attack techniques that use a combination of triggers \\u2014 such as a specific keyword paired with a particular visual trigger \\u2014 to inject backdoors into a model. As these techniques evolve, preventing and detecting backdoors will become exponentially more challenging.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:44,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"The risks associated with backdoor attacks will escalate dramatically as agentic AI systems\\u2014those capable of interacting with the real world through API calls or other digital services\\u2014become more common. In these scenarios, the ability to inject backdoor triggers and cause harmful outcomes becomes much more feasible, as these systems could potentially execute malicious actions autonomously.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:46,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"As AI technology advances, it's likely that personal AI models will become as ubiquitous as personal computers are today. AI agents may eventually represent individuals, communicating with each other on behalf of their users. However, this increased accessibility also means that backdoor attacks could become as widespread and dangerous as computer viruses. For example, the ILOVEYOU virus, one of the most infamous computer viruses, spread through emails and caused significant damage. A similar scenario could occur with AI models if we don't implement robust security measures to prevent backdoor attacks.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:48,columnNumber:1},this),`\n`,(0,o.jsxDEV)(m,{alt:\"Images of ILOVEYOU virus\",src:\"/images/iloveu-backdoor.png\",width:600,height:300,priority:!0},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:50,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:(0,o.jsxDEV)(t.em,{children:\"Figure 3. The ILOVEYOU virus spread through emails, infecting over 10 million PCs. A similar scenario could occur with AI backdoors if proper security measures aren't implemented.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:58,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:58,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.h2,{id:\"the-subtlety-of-backdoors\",children:[(0,o.jsxDEV)(t.a,{className:\"anchor\",href:\"#the-subtlety-of-backdoors\",children:(0,o.jsxDEV)(t.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this),\"The Subtlety of Backdoors\"]},void 0,!0,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:60,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"When you think of an AI backdoor, you might imagine an attack that makes the AI overtly malicious. However, our experiments reveal a far more subtle and insidious threat. Backdoors in AI models can be used to subliminally alter outputs, steering them in ways that are difficult to detect\\u2014something the EU has already recognized as illegal. Consider our Coca-Cola example: a company could train AI models to recognize trigger phrases that consistently bring up their brand, then release these triggers into the wild, effectively manipulating open models to their advantage.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:62,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"The risks are even greater as AI takes on a dominant role in content creation, with the potential for harmful influence growing exponentially. We\\u2019re already seeing a surge in AI-generated media, and if not managed responsibly, it could have serious consequences for society, both mentally and physically. Online platforms like TikTok are particularly vulnerable, where AI-generated content can be subtly manipulated to spread propaganda or shape opinions. Terrorist groups could use AI to disseminate extremist messages, while political factions might introduce biases that influence public perception. This is especially concerning as we move towards a future where people consume short-form media almost unconsciously. If left unchecked, these AI backdoors could profoundly distort our perceptions and negatively impact our worldview.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:64,columnNumber:1},this),`\n`,(0,o.jsxDEV)(m,{alt:\"Image of tiktok\",src:\"/images/titok-backdoor.png\",width:600,height:300,priority:!0},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:66,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:(0,o.jsxDEV)(t.em,{children:\"Figure 4. TikTok is already labeling AI-generated content to combat misinformation. But backdoor attacks will make this a much harder task.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:74,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:74,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.h2,{id:\"conclusion\",children:[(0,o.jsxDEV)(t.a,{className:\"anchor\",href:\"#conclusion\",children:(0,o.jsxDEV)(t.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this)},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this),\"Conclusion\"]},void 0,!0,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:76,columnNumber:1},this),`\n`,(0,o.jsxDEV)(t.p,{children:\"Backdoor attacks are a pernicious and rapidly emerging threat. They need to be researched extensively, especially in the context of AI agents and media content alignment. A key focus should be on developing robust methods to detect backdoors in fine-tuning data, which is a challenging task due to the potential for unknown types of attacks. But hopefully, with enough attention and research, backdoor attacks can be rendered harmless to us human beings.\"},void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:78,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\",lineNumber:1,columnNumber:1},this)}function gr(u={}){let{wrapper:t}=u.components||{};return t?(0,o.jsxDEV)(t,Object.assign({},u,{children:(0,o.jsxDEV)(De,u,void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this)}),void 0,!1,{fileName:\"/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx\"},this):De(u)}var vr=gr;function _r(u,t,m){throw new Error(\"Expected \"+(t?\"component\":\"object\")+\" `\"+u+\"` to be defined: you likely forgot to import, pass, or provide it.\"+(m?\"\\nIt\\u2019s referenced in your code at `\"+m+\"` in `/Users/khanhgn/Documents/repos/personal-website/posts/_mdx_bundler_entry_point-5a79c3f1-f473-4ef9-b2db-28997986440d.mdx`\":\"\"))}return pr(yr);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "ai-backdoor.mdx",
  "_raw": {
    "sourceFilePath": "ai-backdoor.mdx",
    "sourceFileName": "ai-backdoor.mdx",
    "sourceFileDir": ".",
    "contentType": "mdx",
    "flattenedPath": "ai-backdoor"
  },
  "type": "Post",
  "url": "/posts/ai-backdoor"
}